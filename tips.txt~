sudo cp -P cuda/include/cudnn.h /usr/local/cuda/include
sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*

# check version
cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2
nvcc --version
cat /proc/driver/nvidia/version

# zip
zip -r <filename.zip> file1 dir1

# unzip
#-q = quite
unzip -q <filename.zip> -d <dir_to_unzip>

# install java
install default-jre

# install matlab on docker:
The option "javadir" was the right way to investigate!
./install -javadir /usr/lib/jvm/java-7-openjdk/jre/

#Display Information of /home File System
df -hT /home

# Make .Xauthority
$xauth
$xauth>quit
or $touch ~/.Xauthority
$chown <group>:<user> .Xauthority
notice: create .Xauthority for root if use sudo

# Run matlab
should run as normal user, not run by sudo

# run ipython on docker to serve with 0.0.0.0 and diferrent port, and no open browser
ipython notebook --ip=0.0.0.0 --port=80 --no-browser


#install flash ubuntu14
sudo apt-get update
sudo apt-get install adobe-flashplugin

#run crontab & check crontab
Install rsyslog inside the container with apt-get install rsyslog and launch it with the command rsyslogd before starting cron with cron -L15 (maximum logging). 
Then watch the file /var/log/syslog inside the container to see the cron daemon's own log output. 
It will tell you if there was a problem parsing your crontab and it will, 
in your case, log an entry every minute similar to the below if it has registered and is trying to run your job.

#run python script in crontab
=> must check no relative path

#set JAVA_HOME for Hadoop
=> set JAVA_HOME in hadoop-env.sh will work

# zeppelin: set Spark memory
=> first, check MemoryStore: MemoryStore: MemoryStore started with capacity 265.1 MB
=> Solution:
we recommend you to set SPARK_HOME because many spark configurations can be handled better in this way.
SPARK_SUBMIT_OPTIONS in conf/zeppelin-env.sh is valid only when you set SPARK_HOME.
Since you get JVM error, if you have any JVM related configuration in your conf/zeppelin-env.sh can you share it?

If you want to change driver memory without setting SPARK_HOME, you can try ZEPPELIN_INTP_MEM in conf/zeppelin-env.sh file as below:
export ZEPPELIN_INTP_MEM="-Xmx10g" in conf/zeppelin-env.sh file.
FYI, Zeppelin's default ZEPPELIN_INTP_MEM is "-Xmx1024m -XX:MaxPermSize=512m".

Though Spark's UI won't display "Storage Memory" as 10g, because it will shows fraction of java heap to use for memory cache.

I tested both with/without SPARK_HOME setting in yarn-client mode, and Spark UI shows different value.

    With SPARK_HOME setting if I export SPARK_SUBMIT_OPTIONS="--driver-momory 10g", driver storage memory is 5.2G.
    Without SPARK_HOME setting if I export ZEPPELIN_INT_MEM="-Xmx10g", driver storage memory is 4.8G.

The difference here is that first one starts spark interpreter process by running spark_submit and second one runs with java.

# Error no X configuration when install R libraries
Solution: https://ubuntuforums.org/showthread.php?t=787464

